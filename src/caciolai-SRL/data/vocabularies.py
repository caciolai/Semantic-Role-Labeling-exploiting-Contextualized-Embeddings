from typing import *

from collections import Counter
from tqdm import tqdm 

from .utils import json_dump, json_load


class Vocabulary():
    def __init__(self, counter: Counter=None, 
                 specials: List[str]=[], 
                 min_freq: int = 1,
                 only_stoi: bool = False):
        """
        Custom base vocabulary class. 

        Args:
            counter:    Counter object with token occurrences.
            specials:   List of special tokens.
            min_freq:   Minimum number of occurrences for a token to be considered in
                        the vocabulary. (default: 1, all the tokens are considered)
        
        Attributes:
            stoi (dict):    Mapping from token strings (s) to indices (i)
            only_stoi:      Hold only mapping from strings to indices
            itos (dict):    Mapping from indices (i) to token strings (s)
        """

        self.stoi = dict()
        self.only_stoi = only_stoi
        self.itos = None if self.only_stoi else []
        
        if counter is not None:
            idx = 0
            for s in specials:
                self.stoi[s] = idx
                if not self.only_stoi:
                    self.itos.append(s)
                idx += 1

            for tok, freq in counter.items():
                if freq >= min_freq:
                    self.stoi[tok] = idx
                    if not self.only_stoi:
                        self.itos.append(tok)
                    idx += 1
    
    def save(self, path: str, fname: str):
        """
        Saves the vocabulary (its two dictionaries) on disk.

        Args:
            path:   Path of the folder to save into.
            fname:  Name of the file to save onto.
        """
        state = {"stoi": self.stoi, "itos": self.itos}
        json_dump(state, path, fname)
    
    def load(self, fpath):
        """
        Loads the vocabulary (its two dictionaries) from disk.
        
        Args:
            fpath:  Path of the file to load from.
        """
        state = json_load(fpath)
        self.stoi = {s: int(i) for s, i in state["stoi"].items()}
        if not self.only_stoi:
            self.itos = [s for s in state["itos"]]
        else:
            self.itos = None

    def __getitem__(self, key):
        if type(key) == str:
            return self.stoi[key]
        elif not self.only_stoi and type(key) == int:
            return self.itos[key]
        elif self.only_stoi and type(key) == int:
            raise NotImplementedError("This vocabulary holds only the mapping from strings to indices")
        else:
            raise KeyError("{} is neither a token nor an index".format(key))
    
    def __len__(self):
        return len(self.itos)
    
    def __contains__(self, key):
        if type(key) == str:
            return key in self.stoi
        elif not self.only_stoi and type(key) == int:
            return key >= 0 and key < len(self.itos)
        elif self.only_stoi and type(key) == int:
            raise NotImplementedError("This vocabulary holds only the mapping from strings to indices")
        else:
            raise KeyError("{} is neither a token nor an index".format(key))

class PredicateFramesVocabulary(Vocabulary):
    def __init__(self, counter: Counter=None, specials: List[str]=[], min_freq: int = 1):
        """
        Vocabulary to hold the predicate frames.

        Args:
            see base class.
        """
        super().__init__(counter, specials, min_freq)

class SemanticRolesVocabulary(Vocabulary):
    def __init__(self, counter: Counter=None, specials: List[str]=[], min_freq: int = 1):        
        """
        Vocabulary to hold the predicate frames.

        Args:
            see base class.
        """
        super().__init__(counter, specials, min_freq)
    
    def encode(self, labels: List[str]) -> List[int]:
        """
        Encodes a list of tokens (semantic roles) into a list of indices.

        Args:
            labels: List of semantic role labels for the sentence
        Returns:
            The list of indices corresponding to the input semantic role labels.
        """
        indices = list()
        for label in labels:
            if label in self.stoi: # vocabulary string to integer
                indices.append(self[label])
            else:
                raise KeyError("Dictionary incomplete")
        return indices        

class WordsVocabulary(Vocabulary):
    def __init__(self, counter: Counter=None, specials: List[str]=[], min_freq: int = 1):
        """
        Vocabulary to hold the words.

        Args:
            see base class.
        """
        super().__init__(counter, specials, min_freq)
    
    def encode(self, words: List[str]) -> List[int]:
        """
        Encodes a list of tokens (words) into a list of indices.

        Args:
            words: List of words for the sentence.
        Returns:
            The list of indices corresponding to the input words.
        """
        indices = list()
        for word in words:
            if word in self.stoi: # vocabulary string to integer
                indices.append(self[word])
            else:
                indices.append(self["<unk>"])
        return indices

class LemmasVocabulary(Vocabulary):
    def __init__(self, counter: Counter=None, specials: List[str]=[], min_freq: int = 1):
        """
        Vocabulary to hold the lemmas.

        Args:
            see base class.
        """
        super().__init__(counter, specials, min_freq)
    
    def encode(self, lemmas: List[str]) -> List[int]:
        """
        Encodes a list of tokens (lemmas) into a list of indices.

        Args:
            lemmas: List of semantic lemmas for the sentence
        Returns:
            The list of indices corresponding to the input lemmas.
        """
        indices = list()
        for lemma in lemmas:
            if lemma in self.stoi: # vocabulary string to integer
                indices.append(self[lemma])
            else:
                indices.append(self["<unk>"])
        return indices

class POSTagsVocabulary(Vocabulary):
    def __init__(self, counter: Counter=None, specials: List[str]=[], min_freq: int = 1):
        """
        Vocabulary to hold the POS tags.

        Args:
            see base class.
        """
        super().__init__(counter, specials, min_freq)
    
    def encode(self, tags: List[str]) -> List[int]:
        """
        Encodes a list of tokens (POS tags) into a list of indices.

        Args:
            tags: List of POS tags for the sentence.
        Returns:
            The list of indices corresponding to the input pos tags.
        """
        indices = list()
        for tag in tags:
            if tag in self.stoi: # vocabulary string to integer
                indices.append(self[tag])
            else:
                indices.append(self["<unk>"])
        return indices                


def build_words_vocab(dataset: List[dict], min_freq: int=1) -> WordsVocabulary:
    """
    Given a raw dataset builds a WordsVocabulary.

    Args:
        dataset:    a dataset in the form returned by load_data.
        min_freq:   to ignore tokens with frequency below a certain threshold.
    
    Returns:
        A vocabulary for the words.
    """
    counter = Counter()
    for i in tqdm(range(len(dataset)), desc="Building vocabulary"):
        item = dataset[i]
        for word in item["words"]:
            counter[word]+=1

    # we add special tokens for handling padding and unknown words at testing time.
    return WordsVocabulary(counter, specials=['<pad>', '<unk>'], min_freq=min_freq)

def build_lemmas_vocab(dataset: List[dict], min_freq: int=1) -> LemmasVocabulary:
    """
    Given a raw dataset builds a LemmasVocabulary.

    Args:
        dataset:    a dataset in the form returned by load_data.
        min_freq:   to ignore tokens with frequency below a certain threshold.
    
    Returns:
        A vocabulary for the lemmas.
    """
    counter = Counter()
    for i in tqdm(range(len(dataset)), desc="Building vocabulary"):
        item = dataset[i]
        for lemma in item["lemmas"]:
            counter[lemma]+=1

    # we add special tokens for handling padding and unknown words at testing time.
    return LemmasVocabulary(counter, specials=['<pad>', '<unk>'], min_freq=min_freq)

def build_pos_vocab(dataset: List[dict], min_freq: int=1) -> POSTagsVocabulary:
    """
    Given a raw dataset builds a POSTagsVocabulary.

    Args:
        dataset:    a dataset in the form returned by load_data.
        min_freq:   to ignore tokens with frequency below a certain threshold.
    
    Returns:
        A vocabulary for the POS tags.
    """
    counter = Counter()
    for i in tqdm(range(len(dataset)), desc="Building vocabulary"):
        item = dataset[i]
        for tag in item["pos_tags"]:
            counter[tag]+=1

    # we add special tokens for handling padding and unknown words at testing time.
    return POSTagsVocabulary(counter, specials=['<pad>', '<unk>'], min_freq=min_freq)

def build_srl_vocab(srl_path: str) -> SemanticRolesVocabulary:
    """
    Builds a vocabulary for the semantic role labels in dataset.

    Args:
        dataset: The dataset with the considered corpus.
    """
    counter = Counter()
    with open(srl_path, "r") as f:
        for line in f:
            try:
                va_roles = line.strip().split("\t")[1:]
                for role in va_roles:
                    if role != "_":
                        counter[role] += 1
            except ValueError as err:
                continue # skipping initial line
    
    additional_roles = {'Connective', 'Recursive', 'Modal', 'Negation', 'Predicative', 'Modifier'} 
    for role in additional_roles:
        counter[role] += 1
                
    return SemanticRolesVocabulary(counter, specials=['<pad>', '_'])

def build_pf_vocab(pf_path: str) -> PredicateFramesVocabulary:
    """
    Builds a vocabulary for the predicate frames in dataset.

    Args:
        dataset: The dataset with the considered corpus.
    """
    counter = Counter()
    
    with open(pf_path, "r") as f:
        for line in f:
            try:
                _, va_predicate = line.strip().split("\t")
                # m = re.search("va:([\d]+)f", va_predicate_id)
                # predicate_id = int(m.group(1))
                counter[va_predicate] += 1
            except ValueError as err:
                continue # skipping initial line
                
    return PredicateFramesVocabulary(counter, specials=['<pad>', '_'])

