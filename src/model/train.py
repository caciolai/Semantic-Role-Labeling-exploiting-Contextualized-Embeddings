from typing import *

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import precision_score, recall_score, f1_score

import tqdm

from ..data.vocabularies import PredicateFramesVocabulary, SemanticRolesVocabulary
from ..data.preprocessing import prepare_dataloaders_34, prepare_dataloaders_234
from ..data.utils import json_dump
from .models import *
from .serialization import save_model

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
model_folder_path = "../../model"

class Trainer():
    """Utility class to train and evaluate a model."""

    def __init__(self, model, loss_function, optimizer, device,
                 verbose_level=2, verbose_interval=100):
        """
        Args:
            model:              The model we want to train.
            loss_function:      The loss function to minimize.
            optimizer:          The optimizer used to minimize loss_function.
            device:             The device to train on.
            include_chars:      Whether to include character-level embeddings. 
            include_pos:        Whether to include POS embeddings.     
            verbose_level:      The level of verbosity for logging progress.
            verbose_interval:   The number of steps between logs.
        """
        self.model = model
        self.loss_function = loss_function
        self.optimizer = optimizer
        self.device = device
        self.verbose_level = verbose_level
        self.verbose_interval = verbose_interval

        self.model.to(self.device) 

    def f1_score(self, y_true, y_pred, srl_vocab):
        y_true = y_true.detach().cpu()
        y_pred = y_pred.detach().cpu()
        y_pred = torch.argmax(y_pred, -1)
            
        valid_indices = y_true != srl_vocab['<pad>']
        valid_labels = y_true[valid_indices]
        valid_predictions = y_pred[valid_indices]

        y_true = valid_labels.numpy()
        y_pred = valid_predictions.numpy()

        batch_f1_score = f1_score(y_true, y_pred, average='macro')
        return batch_f1_score


class Trainer34(Trainer):
    def __init__(self, model, loss_function, optimizer, device,
                 verbose_level=2, verbose_interval=100):
        
        """Utility class to train Model34."""
        super().__init__(model, loss_function, optimizer, device,
                 verbose_level, verbose_interval)


    def train(self, train_dataset:DataLoader, dev_dataset:DataLoader, epochs:int=1,
              collect_f1_score=True,
              early_stopping=True, min_delta=1e-4, patience=3) -> dict:
        """
        Trains the model for the specified number of epochs.
        Performs early stopping by checking the loss on a dev dataset.

        Args:
            train_dataset:      A Dataset or DataLoader instance containing
                                the training instances.
            dev_dataset:        A Dataset or DataLoader instance used to evaluate
                                learning progress.
            epochs:             The number of times to iterate over train_dataset.
            collect_f1_score:   Whether to keep track of f1 score.
            early_stopping:     Whether to perform early stopping.
            min_delta:          Minimum increment in dev loss to consider 
                                the model performance as improving.
            patience:           How many epochs without improvement to tolerate.

        Returns:
            history: The training history in terms of loss (and optionally f1 score)
                    on both train and dev dataset
        """
        
        assert epochs > 1 and isinstance(epochs, int)
        if self.verbose_level > 0:
            print('Training...')
        
        n_batches = len(train_dataset)
        model_state_checkpoint = None
        bad_dev_epochs = 0
        best_dev_loss = np.inf
        history = {
            "train": {
                "loss": [],
                "f1_score": [],
            },
            "dev": {
                "loss": [],
                "f1_score": [],
            }
        }

        try:
            for epoch in range(epochs):
                epoch_loss = 0.0
                epoch_precision = 0.0
                epoch_recall = 0.0
                epoch_f1_score = 0.0
                
                self.model.train()
                
                batch_iterator = enumerate(train_dataset)
                if self.verbose_level > 0:
                    # track progress via tqdm
                    batch_iterator = tqdm(batch_iterator, total=n_batches, 
                            desc="Train Epoch {:d}/{:d}".format(epoch+1, epochs))
                
                for step, batch in batch_iterator:
                    # for each minibatch fetch the data and labels

                    lengths = batch["length"].to(self.device)
                    pred_frame = batch["pred_frame"].to(self.device)
                    pred_pos = batch["pred_pos"].to(self.device)
                    pred_mask = batch["pred_mask"].to(self.device)
                    word = batch["word"].to(self.device)
                    lemma = batch["lemma"].to(self.device)
                    pos_tag = batch["pos_tag"].to(self.device)
                    bert_sent = batch["bert_sent"].to(self.device)
                    
                    y_true = batch["srl"].to(self.device)

                    self.optimizer.zero_grad()
                    
                    # obtain model predictions
                    y_pred = self.model(lengths, pred_frame, pred_pos, pred_mask, 
                                        word, lemma, pos_tag, bert_sent)

                    y_pred = y_pred.view(-1, y_pred.shape[-1])
                    y_true = y_true.view(-1)
                    
                    # compute loss
                    batch_loss = self.loss_function(y_pred, y_true)

                    # backprop and optimize
                    batch_loss.backward()
                    self.optimizer.step()

                    epoch_loss += batch_loss.detach().cpu().item()
                    if collect_f1_score:
                        epoch_f1_score += self.f1_score(y_true, y_pred)

                    if self.verbose_level > 1 and step > 0 and step % self.verbose_interval == 0:
                        avg_loss = epoch_loss / (step+1)
                        # log progress
                        batch_iterator.set_postfix_str("avg loss={:.4f}".format(avg_loss))
                
                epoch_loss = epoch_loss / n_batches
                epoch_f1_score = epoch_f1_score / n_batches
                
                # perform a dev epoch to check overfitting
                dev_loss, dev_f1_score = self.evaluate(dev_dataset)
                history["train"]["loss"].append(epoch_loss)
                history["dev"]["loss"].append(dev_loss)

                if collect_f1_score:
                    history["train"]["f1_score"].append(epoch_f1_score)
                    history["dev"]["f1_score"].append(dev_f1_score)

                # Handling early stopping
                if early_stopping:
                    if best_dev_loss - dev_loss >= min_delta:
                        bad_dev_epochs = 0
                        model_state_checkpoint = self.model.state_dict()
                        best_dev_loss = dev_loss

                    elif best_dev_loss - dev_loss < min_delta and bad_dev_epochs < patience - 1:
                        bad_dev_epochs += 1
                        print(f"Bad dev epoch, patience for another {patience - bad_dev_epochs} epoch(s).")

                    elif best_dev_loss - dev_loss < min_delta and bad_dev_epochs == patience - 1:
                        print(f"Training interrupted by early stopping after {epoch+1} epochs.")
                        print("Restoring model state.")
                        self.model.load_state_dict(model_state_checkpoint)
                        break
        
        except KeyboardInterrupt:
            print("Training interrupted by user.")
        
        if self.verbose_level > 0:
            print('Done!')
        
        return history
    

    def evaluate(self, dev_dataset: DataLoader,
                collect_f1_score=True) -> float:
        """
        Performs a dev epoch to check overfitting.

        Args:
            dev_dataset: The dataset to use to evaluate the model.
            collect_f1_score: Whether to keep track of f1 score.

        Returns:
            The average loss over dev_dataset.
        """
        assert all([param is not None for param in [
                self.device, self.verbose_level, self.verbose_interval]])
        
        dev_loss = 0.0
        dev_f1_score = 0.0

        self.model.eval()

        n_batches = len(dev_dataset)

        with torch.no_grad():
            batch_iterator = enumerate(dev_dataset)
            if self.verbose_level > 0:
                batch_iterator = tqdm(batch_iterator, total=n_batches, desc="Dev Epoch")
                
            for step, batch in batch_iterator:
                lengths = batch["length"].to(self.device)
                pred_frame = batch["pred_frame"].to(self.device)
                word = batch["word"].to(self.device)
                lemma = batch["lemma"].to(self.device)
                pos_tag = batch["pos_tag"].to(self.device)
                bert_sent = batch["bert_sent"].to(self.device)
                pred_pos = batch["pred_pos"].to(self.device)
                pred_mask = batch["pred_mask"].to(self.device)
                
                y_true = batch["srl"].to(self.device)

                y_pred = self.model(lengths, pred_frame, pred_pos, pred_mask, 
                                        word, lemma, pos_tag, bert_sent)
                
                y_pred = y_pred.view(-1, y_pred.shape[-1])
                y_true = y_true.view(-1)
                batch_loss = self.loss_function(y_pred, y_true)

                dev_loss += batch_loss.detach().cpu().item()
                if collect_f1_score:
                    dev_f1_score += self.f1_score(y_true, y_pred)

                if self.verbose_level > 1 and step > 0 and step % self.verbose_interval == 0:
                    avg_loss = dev_loss / step
                    batch_iterator.set_postfix_str("avg loss={:.4f}".format(avg_loss))
        
        return dev_loss / len(dev_dataset), dev_f1_score / len(dev_dataset)


class Trainer234(Trainer):
    def __init__(self, model, loss_function, optimizer, device, 
                 verbose_level=2, verbose_interval=100):
    
        """Utility class to train Model234."""
        super().__init__(model, loss_function, optimizer, device,
                 verbose_level, verbose_interval)

    def train(self, train_dataset:DataLoader, dev_dataset:DataLoader, epochs:int=1,
              collect_f1_score=True,
              early_stopping=True, min_delta=1e-4, patience=3) -> float:
        """
        Trains the model for the specified number of epochs.
        Performs early stopping by checking the loss on a dev dataset.

        Args:
            train_dataset:      A Dataset or DataLoader instance containing
                                the training instances.
            dev_dataset:        A Dataset or DataLoader instance used to evaluate
                                learning progress.
            epochs:             The number of times to iterate over train_dataset.
            collect_f1_score:   Whether to keep track of f1 score.
            early_stopping:     Whether to perform early stopping.
            min_delta:          Minimum increment in dev loss to consider 
                                the model performance as improving.
            patience:           How many epochs without improvement to tolerate.

        Returns:
            history: The training history in terms of loss (and optionally f1 score)
                    on both train and dev dataset
        """
        
        assert epochs > 1 and isinstance(epochs, int)
        if self.verbose_level > 0:
            print('Training...')
        
        n_batches = len(train_dataset)
        model_state_checkpoint = None
        bad_dev_epochs = 0
        best_dev_loss = np.inf
        history = {
            "train": {
                "loss": [],
                "f1_score": {
                    "srl": [],
                    "pf": []
                },
            },
            "dev": {
                "loss": [],
                "f1_score": {
                    "srl": [],
                    "pf": []
                },
            }
        }

        try:
            for epoch in range(epochs):
                epoch_loss = 0.0
                epoch_f1_score_srl = 0.0
                epoch_f1_score_pf = 0.0
                self.model.train()
                
                batch_iterator = enumerate(train_dataset)
                if self.verbose_level > 0:
                    # track progress via tqdm
                    batch_iterator = tqdm(batch_iterator, total=n_batches, 
                            desc="Train Epoch {:d}/{:d}".format(epoch+1, epochs))
                
                for step, batch in batch_iterator:
                    # for each minibatch fetch the data and labels
                    self.optimizer.zero_grad()
                               
                    lengths = batch["length"].to(self.device)
                    pred_pos = batch["pred_pos"].to(self.device)
                    pred_mask = batch["pred_mask"].to(self.device)
                    word = batch["word"].to(self.device)
                    lemma = batch["lemma"].to(self.device)
                    pos_tag = batch["pos_tag"].to(self.device)
                    bert_sent = batch["bert_sent"].to(self.device)
                    
                    srl_true = batch["srl"].to(self.device)
                    pf_true = batch["pred_frame"].to(self.device)

                    # obtain (multi-head) model prediction, bot predicates and arguments
                    pf_pred, srl_pred = self.model(lengths, pred_pos, pred_mask, 
                                        word, lemma, pos_tag, bert_sent)
                    
                    srl_pred = srl_pred.view(-1, srl_pred.shape[-1])
                    srl_true = srl_true.view(-1)
                    
                    pf_pred = pf_pred.view(-1, pf_pred.shape[-1])
                    pf_true = pf_true.view(-1)
                    
                    # compute loss as a linear combination with the specified coefficient
                    alpha = self.loss_function["alpha"]
                    batch_loss = (1-alpha) * self.loss_function["srl"](srl_pred, srl_true) + \
                                alpha * self.loss_function["pf"](pf_pred, pf_true)

                    # backprop and optimize
                    batch_loss.backward()
                    self.optimizer.step()

                    epoch_loss += batch_loss.detach().cpu().item()
                    
                    if collect_f1_score:
                        epoch_f1_score_srl += self.f1_score(srl_true, srl_pred)
                        epoch_f1_score_pf += self.f1_score(pf_true, pf_pred)

                    if self.verbose_level > 1 and step > 0 and step % self.verbose_interval == 0:
                        avg_loss = epoch_loss / (step+1)
                        # log progress
                        batch_iterator.set_postfix_str("avg loss={:.4f}".format(avg_loss))
                
                train_loss = epoch_loss / n_batches
                train_f1_score_srl = epoch_f1_score_srl / n_batches
                train_f1_score_pf = epoch_f1_score_pf / n_batches

                # perform a dev epoch to check overfitting
                dev_loss, dev_f1_score_srl, dev_f1_score_pf = self.evaluate(dev_dataset)

                history["train"]["loss"].append(train_loss)
                history["dev"]["loss"].append(dev_loss)

                if collect_f1_score:
                    history["train"]["f1_score"]["srl"].append(train_f1_score_srl)
                    history["train"]["f1_score"]["pf"].append(train_f1_score_pf)
                    history["dev"]["f1_score"]["srl"].append(dev_f1_score_srl)
                    history["dev"]["f1_score"]["pf"].append(dev_f1_score_pf)


                # Handling early stopping
                if early_stopping:
                    if best_dev_loss - dev_loss >= min_delta:
                        bad_dev_epochs = 0
                        model_state_checkpoint = self.model.state_dict()
                        best_dev_loss = dev_loss

                    elif best_dev_loss - dev_loss < min_delta and bad_dev_epochs < patience - 1:
                        bad_dev_epochs += 1
                        print(f"Bad dev epoch, patience for another {patience - bad_dev_epochs} epoch(s).")

                    elif best_dev_loss - dev_loss < min_delta and bad_dev_epochs == patience - 1:
                        print(f"Training interrupted by early stopping after {epoch+1} epochs.")
                        print("Restoring model state.")
                        self.model.load_state_dict(model_state_checkpoint)
                        break
        
        except KeyboardInterrupt:
            print("Training interrupted by user.")
        
        if self.verbose_level > 0:
            print('Done!')
        
        return history
    

    def evaluate(self, dev_dataset: DataLoader,
                collect_f1_score=True) -> float:
        """
        Performs a dev epoch to check overfitting.
        
        Args:
            dev_dataset: The dataset to use to evaluate the model.
            collect_f1_score: Whether to keep track of f1 score.

        Returns:
            The average loss over dev_dataset.
        """
        assert all([param is not None for param in [
                self.device, self.verbose_level, self.verbose_interval]])
        
        dev_loss = 0.0
        dev_f1_score_srl = 0.0
        dev_f1_score_pf = 0.0
        self.model.eval()

        n_batches = len(dev_dataset)

        with torch.no_grad():
            batch_iterator = enumerate(dev_dataset)
            if self.verbose_level > 0:
                batch_iterator = tqdm(batch_iterator, total=n_batches, desc="Dev Epoch")
                
            for step, batch in batch_iterator:
                lengths = batch["length"].to(self.device)
                pred_pos = batch["pred_pos"].to(self.device)
                pred_mask = batch["pred_mask"].to(self.device)
                word = batch["word"].to(self.device)
                lemma = batch["lemma"].to(self.device)
                pos_tag = batch["pos_tag"].to(self.device)
                bert_sent = batch["bert_sent"].to(self.device)
                
                srl_true = batch["srl"].to(self.device)
                pf_true = batch["pred_frame"].to(self.device)

                pf_pred, srl_pred = self.model(lengths, pred_pos, pred_mask, 
                                        word, lemma, pos_tag, bert_sent)
                
                srl_pred = srl_pred.view(-1, srl_pred.shape[-1])
                srl_true = srl_true.view(-1)
                
                pf_pred = pf_pred.view(-1, pf_pred.shape[-1])
                pf_true = pf_true.view(-1)
                
                alpha = self.loss_function["alpha"]
                batch_loss = (1-alpha) * self.loss_function["srl"](srl_pred, srl_true) + \
                            alpha * self.loss_function["pf"](pf_pred, pf_true)

                dev_loss += batch_loss.detach().cpu().item()

                if collect_f1_score:
                    dev_f1_score_srl += self.f1_score(srl_true, srl_pred)
                    dev_f1_score_pf += self.f1_score(pf_true, pf_pred)

                if self.verbose_level > 1 and step > 0 and step % self.verbose_interval == 0:
                    avg_loss = dev_loss / step
                    batch_iterator.set_postfix_str("avg loss={:.4f}".format(avg_loss))
        
        dev_loss = dev_loss / len(dev_dataset)
        dev_f1_score_srl = dev_f1_score_srl / len(dev_dataset)
        dev_f1_score_pf = dev_f1_score_pf /  len(dev_dataset)
        
        return dev_loss, dev_f1_score_srl, dev_f1_score_pf


def train34(
    model: nn.Module,
    hyperparams34: dict,
    srl_vocab: SemanticRolesVocabulary,
    n_epochs 
) -> dict:
    loss_function = nn.CrossEntropyLoss(ignore_index=srl_vocab['<pad>'])

    # advisable to move the model to the proper device before creating optimizer for it
    model.to(DEVICE)
    model_parameters = [param for param in model.parameters() if param.requires_grad]
    optimizer = optim.Adam(model_parameters, lr=hyperparams34["lr"])

    train_dataloader, dev_dataloader, _ = prepare_dataloaders_34()

    trainer = Trainer34(model=model, 
                        loss_function=loss_function, 
                        optimizer=optimizer, 
                        device=DEVICE, 
                        verbose_level=2,
                        verbose_interval=10
                        )
    
    history = trainer.train(train_dataloader, dev_dataloader, n_epochs, 
                            collect_f1_score=True,
                            early_stopping=True, patience=3)

    return history

def train234(
    model: nn.Module,
    hyperparams234: dict,
    srl_vocab: SemanticRolesVocabulary,
    pf_vocab: PredicateFramesVocabulary,
    n_epochs 
) -> dict:

    train_dataloader, dev_dataloader, _ = prepare_dataloaders_234()

    loss_pf = nn.CrossEntropyLoss(ignore_index=pf_vocab['<pad>'])
    loss_srl = nn.CrossEntropyLoss(ignore_index=srl_vocab['<pad>'])
    alpha = len(srl_vocab) / len(pf_vocab)  # importance of pf term relative to srl
    loss_function = {"pf": loss_pf, "srl": loss_srl, "alpha": alpha}

    optimizer = optim.Adam(model.parameters(), lr=hyperparams234["lr"])

    trainer = Trainer234(model=model, 
                        loss_function=loss_function, 
                        optimizer=optimizer, 
                        device=DEVICE, 
                        verbose_level=2,
                        verbose_interval=10
                        )
    history = trainer.train(train_dataloader, dev_dataloader, n_epochs, 
                            collect_f1_score=True,
                            early_stopping=True, patience=3)

    return history

def save_training(
        trainer,
        hyperparams,
        words_vocab,
        lemmas_vocab,
        pos_vocab,
        srl_vocab,
        pf_vocab
    ):
    suffix = "15-07_Model234"

    fname_model = "model_params_{}.pt".format(suffix)
    fname_hparams = "hparams_{}.json".format(suffix)
    fname_srl_vocab = "srl_vocab.json"
    fname_pf_vocab = "pf_vocab.json"
    fname_words_vocab = "words_vocab.json"
    fname_lemmas_vocab = "lemmas_vocab.json"
    fname_pos_vocab = "pos_vocab.json"


    model = trainer.model

    save_model(model, model_folder_path, fname_model)
    json_dump(hyperparams, model_folder_path, fname_hparams)
    
    words_vocab.save(model_folder_path, fname_words_vocab)
    lemmas_vocab.save(model_folder_path, fname_lemmas_vocab)
    pos_vocab.save(model_folder_path, fname_pos_vocab)
    srl_vocab.save(model_folder_path, fname_srl_vocab)
    pf_vocab.save(model_folder_path, fname_pf_vocab)

