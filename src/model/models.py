from typing import *

import torch
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence

class Model34(nn.Module):
    def __init__(self, hparams):
        """
        A BiLSTM model with pre-trained BERT embeddings.

        Args:
            hparams:        Dictionary with the hyperparameters for the model.

        Attributes:
            lstm:           LSTM layer with dropout, can be bidirectional or not.
            dropout:        Dropout layer, with same amount of dropout used in lstm.
            linear:         Linear output layer.
        """
        super().__init__()

        # embedding layers
        self.words_embedding = nn.Embedding(hparams["num_words"],
                                             hparams["words_embedding_dim"])

        self.lemmas_embedding = nn.Embedding(hparams["num_lemmas"],
                                             hparams["lemmas_embedding_dim"])
        
        self.pos_embedding = nn.Embedding(hparams["num_pos_tags"],
                                             hparams["pos_embedding_dim"])
        
        self.pf_embedding = nn.Embedding(hparams["num_pf"], 
                                          hparams["pf_embedding_dim"])

        # hidden linear projection layer with batch normalization
        input_dim = hparams["pf_embedding_dim"] + \
                        hparams["words_embedding_dim"] + \
                        hparams["lemmas_embedding_dim"] + \
                        hparams["pos_embedding_dim"] + \
                        hparams["bert_embedding_dim"]
                        
        self.hidden = nn.Linear(input_dim, hparams["hidden_dim"])
        self.bn = nn.BatchNorm1d(num_features=hparams["hidden_dim"])

        # BiLSTM
        self.lstm = nn.LSTM(hparams["hidden_dim"], hparams["hidden_dim"], 
                            bidirectional=hparams["bidirectional"],
                            batch_first=True,
                            num_layers=hparams["num_layers"],
                            dropout=hparams["dropout"])

        lstm_output_dim = hparams["hidden_dim"] \
                            if hparams["bidirectional"] is False \
                            else 2*hparams["hidden_dim"]

        # dropout and final classification linear projection layer
        self.dropout = nn.Dropout(hparams["dropout"])
        self.classifier = nn.Linear(2*lstm_output_dim, hparams["num_classes"])
    
    def set_pretrained_word_embedding(self, embedding):
        self.words_embedding.weight.data.copy_(embedding)
        self.words_embedding.weight.requires_grad = False
    
    def forward(self, lengths, pred_frame, pred_pos, pred_mask,
                word, lemma, pos_tag, bert_sent):
        """
        Args:
            lengths:    Tensor of lengths of sentences in the batch
                        (batch_size)
            pred_frame: Batch indices vectors pointing to the predicate semantic frame
                        (batch_size)
            pred_pos:   Batch of indices pointing to the predicate position 
                        in the sentences
                        (batch_size)
            pred_mask:  Binary mask to indicate which sentences have predicates and which do not
                        (batch_size)
            word:       Batch of indices for the words of the sentences in the batch
                        (batch_size x seq_len x 1)
            lemma:      Batch of indices for the lemmas of the sentences in the batch
                        (batch_size x seq_len x 1)
            pos_tag:    Batch of indices referring to the pos tags of the tokens in the sentences of the batch
                        (batch_size x seq_len x 1)
            bert_sent:  Batch of sentences embedded by bert
                        (batch_size x seq_len x bert_embedding)
        """
        seq_len = word.shape[1]

        # (batch_size x pf_embedding_dim)
        pf_embeddings = self.dropout(self.pf_embedding(pred_frame))
        # (batch_size x seq_len x pf_embedding_dim)
        pf_embeddings = pf_embeddings.unsqueeze(1).repeat(1, seq_len, 1)

        # (batch_size x seq_len x words_embedding_dim)
        words_embeddings = self.dropout(self.words_embedding(word))

        # (batch_size x seq_len x lemmas_embedding_dim)
        lemmas_embeddings = self.dropout(self.lemmas_embedding(lemma))

        # (batch_size x seq_len x pos_embedding_dim)
        pos_embeddings = self.dropout(self.pos_embedding(pos_tag))
        
        # (batch_size x seq_len x 
        #   (pf_embedding_dim + words_embedding_dim + lemmas_embedding_dim + pos_embedding_dim + bert_embedding_dim))
        x = torch.cat((pf_embeddings, words_embeddings, lemmas_embeddings, pos_embeddings, bert_sent), dim=-1)
        
        # (batch_size x seq_len x hidden_dim)
        x = self.hidden(x)
        x = self.bn(x.transpose(1, 2)).transpose(1, 2)
        x = self.dropout(x)
        
        # packing
        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)
        
        # Fed it to lstm
        # (batch_size x seq_len x 2*hidden_dim)
        o, (h, c) = self.lstm(x)
        o, _ = pad_packed_sequence(o, batch_first=True, padding_value=0)
        lstm_out_dim = o.shape[2]

        # Selecting predicate embedding for all the batch, 
        # to concatenate to each token in the sentence so that
        # the classifier gets pairs (token, predicate)
        
        # (batch_size x 1 x 2*hidden_dim)
        pred_pos = pred_pos.view(-1, 1, 1).repeat(1, 1, lstm_out_dim)
        
        # masking out the samples in the batch with no predicates
        # (batch_size x 1 x 2*hidden_dim)
        pred = o.gather(1, pred_pos)
        pred = pred * pred_mask[:, None, None]

        # (batch_size x seq_len x 2*hidden_dim)
        pred = pred.repeat(1, seq_len, 1)

        # (batch_size x seq_len x 4*hidden_dim)
        o = torch.cat((o, pred), dim=-1)

        o = self.dropout(o)
        o = self.classifier(o)
        return o

class Model234(nn.Module):
    def __init__(self, hparams):
        """
        A BiLSTM model with pre-trained BERT embeddings performing tasks 234:
            - 2: predicate disambiguation
            - 3: roles identification
            - 4: roles classification

        Args:
            hparams:        Dictionary with the hyperparameters for the model.

        Attributes:
            lstm:           LSTM layer with dropout, can be bidirectional or not.
            dropout:        Dropout layer, with same amount of dropout used in lstm.
            linear:         Linear output layer.
        """
        super().__init__()
        
        # embedding layers
        self.words_embedding = nn.Embedding(hparams["num_words"],
                                             hparams["words_embedding_dim"])
        
        self.lemmas_embedding = nn.Embedding(hparams["num_lemmas"],
                                             hparams["lemmas_embedding_dim"])
        
        self.pos_embedding = nn.Embedding(hparams["num_pos_tags"],
                                             hparams["pos_embedding_dim"])
        
        # first hiddden linear projection layer, to feed to predicate BiLSTM
        self.hidden1 = nn.Linear(hparams["words_embedding_dim"] + \
                        hparams["lemmas_embedding_dim"] + \
                        hparams["pos_embedding_dim"] + \
                        hparams["bert_embedding_dim"],
                        hparams["hidden_dim"])

        self.bn1 = nn.BatchNorm1d(num_features=hparams["hidden_dim"])
        
        self.pred_disambiguation_lstm = nn.LSTM(hparams["hidden_dim"], 
                                                hparams["pf_embedding_dim"], 
                                                bidirectional=True,
                                                batch_first=True,
                                                num_layers=hparams["num_layers"],
                                                dropout=hparams["dropout"])
        
        # predicate disambiguation classifier
        pf_embedding_dim = hparams["num_layers"]*hparams["pf_embedding_dim"] \
                            if hparams["bidirectional"] is False \
                            else 2*hparams["num_layers"]*hparams["pf_embedding_dim"]

        self.pred_disambiguation = nn.Linear(pf_embedding_dim, hparams["num_pf"])
        
        # second hiddden linear projection layer, to feed to main BiLSTM 
        # (also with predicate frames embeddings)
        
        self.hidden2 = nn.Linear(hparams["words_embedding_dim"] + \
                        hparams["lemmas_embedding_dim"] + \
                        hparams["pos_embedding_dim"] + \
                        hparams["bert_embedding_dim"] + \
                        pf_embedding_dim, 
                        hparams["hidden_dim"])
        
        self.bn2 = nn.BatchNorm1d(num_features=hparams["hidden_dim"])

        self.lstm = nn.LSTM(hparams["hidden_dim"], hparams["hidden_dim"], 
                            bidirectional=True,
                            batch_first=True,
                            num_layers=hparams["num_layers"],
                            dropout=hparams["dropout"])

        # dropout and final classification linear projection layer
        lstm_output_dim = hparams["hidden_dim"] \
                            if hparams["bidirectional"] is False \
                            else 2*hparams["hidden_dim"]
        self.dropout = nn.Dropout(hparams["dropout"])
        self.classifier = nn.Linear(2*lstm_output_dim, hparams["num_classes"])
    
    def set_pretrained_word_embedding(self, embedding):
        self.words_embedding.weight.data.copy_(embedding)
        self.words_embedding.weight.requires_grad = False
    
    def forward(self, lengths, pred_pos, pred_mask, 
                word, lemma, pos_tag, bert_sent):
        """
        Args:
            lengths:    Tensor of lengths of sentences in the batch
                        (batch_size)
                        (batch_size x seq_len x 1)
            word:       Batch of indices for the words of the sentences in the batch
                        (batch_size x seq_len x 1)
            pred_pos:   Batch of indices pointing to the predicate position 
                        in the sentences
                        (batch_size)
            pred_mask:  Binary mask to indicate which sentences have predicates and which do not
                        (batch_size)
            lemma:      Batch of indices for the lemmas of the sentences in the batch
                        (batch_size x seq_len x 1)
            pos_tag:    Batch of indices referring to the pos tags of the tokens in the sentences of the batch
                        (batch_size x seq_len x 1)
            bert_sent:  Batch of sentences embedded by bert
                        (batch_size x seq_len x bert_embedding)
        """
        batch_size = word.shape[0]
        seq_len = word.shape[1]

        # (batch_size x seq_len x words_embedding_dim)
        words_embeddings = self.dropout(self.words_embedding(word))
        
        # (batch_size x seq_len x lemmas_embedding_dim)
        lemmas_embeddings = self.dropout(self.lemmas_embedding(lemma))

        # (batch_size x seq_len x pos_embedding_dim)
        pos_embeddings = self.dropout(self.pos_embedding(pos_tag))
        
        # (batch_size x seq_len x 
        #   (lemmas_embedding_dim + pos_embedding_dim + bert_embedding_dim))
        x = torch.cat((words_embeddings, lemmas_embeddings, pos_embeddings, bert_sent), dim=-1)
        
        # (batch_size x seq_len x hidden_dim)
        x = self.hidden1(x)
        x = self.bn1(x.transpose(1, 2)).transpose(1, 2)
        x = self.dropout(x)
        
        x_pack = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)
        o, (h, c) = self.pred_disambiguation_lstm(x_pack)
        o, _ = pad_packed_sequence(o, batch_first=True, padding_value=0)

        # Taking last hidden state as predicate frames embeddings
        pf_embeddings = h.transpose(0, 1).reshape(batch_size, -1)
        pf_embeddings = self.dropout(pf_embeddings)

        pred_frames = self.pred_disambiguation(pf_embeddings)
        pf_embeddings = pf_embeddings.unsqueeze(1).repeat(1, seq_len, 1)

        # (batch_size x seq_len x 
        #   (bert_embedding_dim + pf_embedding_dim + lemmas_embedding_dim + pos_embedding_dim))
        x = torch.cat((pf_embeddings, words_embeddings, lemmas_embeddings, pos_embeddings, bert_sent), dim=-1)
        
        x = self.hidden2(x)
        # (batch_size x seq_len x hidden_dim)
        x = self.bn2(x.transpose(1, 2)).transpose(1, 2)
        x = self.dropout(x)

        # (batch_size x seq_len x 2*hidden_dim)
        x_pack = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)
        o, (h, c) = self.lstm(x_pack)
        o, _ = pad_packed_sequence(o, batch_first=True, padding_value=0)
        lstm_out_dim = o.shape[2]

        # Selecting predicate embedding for all the batch, 
        # to concatenate to each token in the sentence so that
        # the classifier gets pairs (token, predicate)
        
        # (batch_size x 1 x 2*hidden_dim)
        pred_pos = pred_pos.view(-1, 1, 1).repeat(1, 1, lstm_out_dim)
        
        # masking out the samples in the batch with no predicates
        # (batch_size x 1 x 2*hidden_dim)
        pred = o.gather(1, pred_pos)
        pred = pred * pred_mask[:, None, None]

        # (batch_size x seq_len x 2*hidden_dim)
        pred = pred.repeat(1, seq_len, 1)

        # (batch_size x seq_len x 4*hidden_dim)
        o = torch.cat((o, pred), dim=-1)

        o = self.dropout(o)
        roles = self.classifier(o)
        
        return pred_frames, roles


