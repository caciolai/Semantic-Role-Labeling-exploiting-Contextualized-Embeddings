from typing import *

import os

from torch.utils.data import DataLoader

from .verbatlas import get_verbatlas_roles_path, get_verbatlas_predicate_ids_path
from .embeddings import bert_embed_data, get_bert_embedder, get_bert_models
from .datasets import load_data, CustomDataset, collate
from .vocabularies import *

data_folder_path = "../../data"
model_folder_path = "../../model"
embeddings_folder = os.path.join(model_folder_path, "embeddings")

def prepare_datasets() -> Tuple[CustomDataset, CustomDataset, CustomDataset]:
    train_data = load_data(data_folder_path, "train.json")
    dev_data = load_data(data_folder_path, "dev.json")
    test_data = load_data(data_folder_path, "test.json")

    # build vocabularies from training data
    words_vocab = build_words_vocab(train_data)
    print(f"There are {len(words_vocab)} different words in the vocabulary")
    
    lemmas_vocab = build_lemmas_vocab(train_data)
    print(f"There are {len(lemmas_vocab)} different lemmas in the vocabulary")

    pos_vocab = build_pos_vocab(train_data)
    print(f"There are {len(pos_vocab)} different POS tags in the vocabulary")

    va_roles_ids_path = get_verbatlas_roles_path()
    srl_vocab = build_srl_vocab(va_roles_ids_path)
    print(f"There are {len(srl_vocab)} possible semantic role labels")  # should be 35

    va_predicate_ids_path = get_verbatlas_predicate_ids_path()
    pf_vocab = build_pf_vocab(va_predicate_ids_path)
    print(f"There are {len(pf_vocab)} possible predicate frames") # should be 474

    train_dataset = CustomDataset(train_data)
    dev_dataset = CustomDataset(dev_data)
    test_dataset = CustomDataset(test_data)

    bert_embedder = get_bert_embedder()

    train_bert_embeddings = bert_embed_data(bert_embedder, train_dataset)
    dev_bert_embedings = bert_embed_data(bert_embedder, dev_dataset)
    test_bert_embeddings = bert_embed_data(bert_embedder, test_dataset)

    train_dataset.encode_data(srl_vocab, pf_vocab, words_vocab, lemmas_vocab, pos_vocab,
                         train_bert_embeddings)

    dev_dataset.encode_data(srl_vocab, pf_vocab, words_vocab, lemmas_vocab, pos_vocab,
                         dev_bert_embedings)

    test_dataset.encode_data(srl_vocab, pf_vocab, words_vocab, lemmas_vocab, pos_vocab,
                         test_bert_embeddings)

    return train_dataset, dev_dataset, test_dataset


def prepare_dataloaders_34(hyperparams34) -> Tuple[DataLoader, DataLoader, DataLoader]:
    train_dataset, dev_dataset, test_dataset = prepare_datasets()

    train_dataloader = DataLoader(train_dataset, hyperparams34["batch_size"], collate_fn=collate, shuffle=True)
    dev_dataloader = DataLoader(dev_dataset, hyperparams34["batch_size"], collate_fn=collate)
    test_dataloader = DataLoader(test_dataset, hyperparams34["batch_size"], collate_fn=collate)

    return train_dataloader, dev_dataloader, test_dataloader