from typing import *

import numpy as np
import torch
from transformers import BertTokenizer, BertModel
from transformers import WEIGHTS_NAME, CONFIG_NAME
BERT_MODEL_NAME = 'bert-base-cased'
BERT_EMBEDDER = None

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

import os
from tqdm import tqdm
import urllib
import zipfile

from .vocabularies import Vocabulary
from .datasets import CustomDataset

data_folder_path = "../../data"
model_folder_path = "../../model"
embeddings_folder = os.path.join(model_folder_path, "embeddings")
bert_folder_path = os.path.join(model_folder_path, "bert")

##################
# WORD EMBEDDINGS
##################
def create_word_embedding_tensor(vocabulary: Vocabulary, embedding_dim: int,
                            pretrained_embeddings: Dict[str, np.ndarray]) -> torch.Tensor:
    """
    Creates a lookup tensor for the tokens in the vocabulary starting from pretrained embeddings.

    Args:
        vocabulary:             The vocabulary with the mapping from tokens to indices.
        embedding_dim:          The dimension of the vectors of the embeddings.
        pretrained_embeddings:  The pretrained embeddings for the tokens.
    
    Returns:
        The lookup tensor of shape (vocabulary length, embedding dimension) 
        with the available pretrained embeddings for the tokens in the vocabulary
    """
    embedding_tensor = torch.randn(len(vocabulary), embedding_dim)
    initialised = 0
    iterator = tqdm(enumerate(vocabulary.itos), desc="Building lookup table of embeddings")
    for i, w in iterator:
        if w not in pretrained_embeddings:
            # check needed for <pad>, <unk> tokens
            continue
        
        initialised += 1
        vec = pretrained_embeddings[w]
        embedding_tensor[i] = torch.from_numpy(vec)         
        
    embedding_tensor[vocabulary["<pad>"]] = torch.zeros(embedding_dim)
    print("Initialised embeddings {}".format(initialised))
    print("Randomly initialised embeddings {} ".format(len(vocabulary) - initialised))
    return embedding_tensor

def get_word_embeddings(emb_fpath: str, vocab: Vocabulary, emb_dim: int) -> torch.Tensor:
    emb_dict = dict()
    with open(emb_fpath, "r") as f:
        for l in tqdm(f, desc="Loading pretrained word embeddings"):
            line = l.split()
            if len(line) == 2:
                # fasttext has an header to be skipped
                continue
            tok = "".join(line[:-emb_dim])
            if tok in vocab.stoi.keys():
                vec = np.array(line[-emb_dim:], dtype=np.float32)
                emb_dict[tok] = vec
    
    return create_word_embedding_tensor(vocab, emb_dim, emb_dict)

def download_embeddings(embeddings_fpath: str):
    url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip'
    filehandle, _ = urllib.request.urlretrieve(url)
    zip_file_object = zipfile.ZipFile(filehandle, 'r')
    first_file = zip_file_object.namelist()[0]
    file = zip_file_object.open(first_file)
    content = file.read()

    with open(embeddings_fpath, "w+") as f:
        f.write(content)

    file.close()

def prepare_embeddings(words_vocabulary: Vocabulary) -> torch.Tensor:

    embeddings_fname = "wiki-news-300d-1M.vec"
    embeddings_fpath = os.path.join(embeddings_folder, embeddings_fname)
    embedding_dim = 300

    if not os.path.isfile(embeddings_fpath):
        print("Downloading pre-trained word embeddings...")
        download_embeddings(embeddings_fpath)
        print("Done!")

    pretrained_embeddings = get_word_embeddings(embeddings_fpath, words_vocabulary, embedding_dim)

    return pretrained_embeddings


##################
# BERT EMBEDDINGS
##################
class BERTEmbedder:
    def __init__(self, bert_model:BertModel, 
               bert_tokenizer:BertTokenizer, 
               device:str):
        """
        Utility class to compute the BERT embeddings of sentences.

        Args
            bert_model:     The pretrained BERT model.
            bert_tokenizer: The pretrained BERT tokenizer.
            device:         Cpu or cuda
        """
        super().__init__()
        self.bert_model = bert_model
        self.bert_model.to(device)
        self.bert_model.eval()
        self.bert_tokenizer = bert_tokenizer
        self.device = device

    def embed_sentence(self, sentence: List[str]) -> torch.FloatTensor:
        """
        Computes BERT embeddings of a sentence.

        Args
            sentence: the sentence to embed.
        
        Returns
            the embedding of the sentence.
        """
        tokenized_sentence, to_merge_wordpieces = self._tokenize_sentence(sentence)
        with torch.no_grad():
            # BERT expects list of sentences and returns tuple of 12 hidden states
            tokenized_sentence = tokenized_sentence.unsqueeze(0)
            _, _, hidden_states = self.bert_model(tokenized_sentence)
        
        # BERT authors report 'Weighted Sum Last Four Hidden' to be one of the
        # best performing feature-based approach for bert-base, so i will sum
        # the output of the last four 
        bert_embeddings = torch.stack(
            [h.squeeze(0).detach() for h in hidden_states[-4:]], 
        axis=0)
        bert_embeddings = torch.sum(bert_embeddings, axis=0)
        bert_embeddings = self._merge_embeddings(bert_embeddings, to_merge_wordpieces)
        
        return bert_embeddings
  
    def _tokenize_sentence(self, sentence: List[str]) \
            -> Tuple[torch.LongTensor, List[List[int]]]:
        """
        Tokenizes a sentence into a format accepted by BERT:
            [CLS] wordpieces [SEP]
        
        Args
            sentence: the sentence to be tokenized.
        
        Returns
            The tokenized sentence and a mapping to reconstruct words from their wordpieces.
            Tokenized sentence has shape (n_wordpieces x bert_embedding_dim).
        """
        # special [CLS] token at the beginning
        tokenized_sentence = [self.bert_tokenizer.cls_token_id]
        
        # BERT expects WordPieces so i tokenize a word at a time and keep track
        # of which wordpieces belong to the same word to be able to merge their embeddings
        # aftwerwards

        wordpieces = []
        for word in sentence:
            encoded_word = self.bert_tokenizer.tokenize(word)
            wordpieces.append([i for i in range(len(tokenized_sentence)-1, len(tokenized_sentence)+len(encoded_word)-1)]) 
            tokenized_sentence.extend(self.bert_tokenizer.convert_tokens_to_ids(encoded_word))
        
        # special [SEP] token at the end
        tokenized_sentence.append(self.bert_tokenizer.sep_token_id)
        tokenized_sentence = torch.LongTensor(tokenized_sentence).to(self.device)
        return tokenized_sentence, wordpieces

    # aggregated_layers has shape: shape batch_size x sequence_length x hidden_size
    def _merge_embeddings(self, bert_embeddings: torch.FloatTensor,
                            wordpieces: List[int]) -> torch.FloatTensor:
        """
        Merges the embeddings of wordpieces belonging to the same word to obtain
        a sentence embedding that has shape compatible with the original sentence.

        Args
            bert_embeddings:    the BERT embeddings of the sentence made of wordpieces
            wordpieces:         the mapping from words to wordpieces
        
        Returns 
            The final embedding of the sentence, of shape
            (sentence_len x bert_embedding_dim)
        """
        
        merged_embeddings = []
        # remove the special [CLS] and [SEP] tokens from the output embeddings
        bert_embeddings = bert_embeddings[1:-1 ,:]
        # for each word i obtain a single embedding by averaging the embeddings
        # of the embeddings of its wordpieces
        for wordpiece in wordpieces:
            word_embeddings = torch.stack([bert_embeddings[i, :] for i in wordpiece], axis=0)
            word_embeddings = torch.mean(word_embeddings, axis=0)
            merged_embeddings.append(word_embeddings)
        
        merged_embeddings = torch.stack(merged_embeddings)
        return merged_embeddings

def download_bert_models():
    # Downloading pre-trained BERT models
    bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)
    bert_model = BertModel.from_pretrained(BERT_MODEL_NAME, output_hidden_states=True)

    # Save a model, configuration and vocabulary that you have fine-tuned

    # If we have a distributed model, save only the encapsulated model
    # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)
    model_to_save = bert_model.module if hasattr(bert_model, 'module') else bert_model

    # If we save using the predefined names, we can load using `from_pretrained`
    output_model_file = os.path.join(bert_folder_path, WEIGHTS_NAME)
    output_config_file = os.path.join(bert_folder_path, CONFIG_NAME)

    torch.save(model_to_save.state_dict(), output_model_file)
    model_to_save.config.to_json_file(output_config_file)
    bert_tokenizer.save_pretrained(bert_folder_path)

def get_bert_models():
    try:
        bert_model = BertModel.from_pretrained(bert_folder_path, output_hidden_states=True)
        bert_tokenizer = BertTokenizer.from_pretrained(bert_folder_path)  # Add specific options if needed
    except:
        download_bert_models()
    finally:
        bert_model = BertModel.from_pretrained(bert_folder_path, output_hidden_states=True)
        bert_tokenizer = BertTokenizer.from_pretrained(bert_folder_path)

        return bert_model, bert_tokenizer

def get_bert_embedder():
    global BERT_EMBEDDER

    if BERT_EMBEDDER is None:
        bert_model, bert_tokenizer = get_bert_models()
        BERT_EMBEDDER = BERTEmbedder(bert_model, bert_tokenizer, DEVICE)
    
    return BERT_EMBEDDER

def bert_embed_data(dataset: CustomDataset) -> torch.FloatTensor:
    """
    Uses a BERTEmbedder to embed data in a dataset.

    Args:
        dataset:        raw dataset.
    
    Returns:
        Bert embeddings.
    """

    bert_embedder = get_bert_embedder()
    bert_embeddings = []
    for i in tqdm(range(len(dataset)), desc="Embedding sentences"):
        item = dataset.get_raw_element(i)
        sent = item["words"]
        embedded_sentence = bert_embedder.embed_sentence(sent)
        bert_embeddings.append(embedded_sentence)
    
    return bert_embeddings

